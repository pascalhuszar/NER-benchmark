{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flair_fintuned_Conll_03_different_embedd.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EXqWPb5aLd9J",
        "GZacepc8LhqZ",
        "r5z0MJFDY72k",
        "tJoKW37iUsyY",
        "tgqgjdvlgM-q",
        "jEmXUeRogl92",
        "tDblEo_YZ2hd",
        "IUp-34p9aqz5",
        "p4iWznPwUxO0",
        "6FnOfvlQbpTb",
        "LEkCXfLrZIz8",
        "aHFumxgxbvAF",
        "_oTyHf5jZblf",
        "sMxCCGWWb772",
        "eEinDdOReGtm",
        "i281Wbd5ejlG",
        "u4icwuEtesow",
        "urbljo5SiHLg",
        "veiZOL0ijsD9",
        "ijJcsDyDj6I2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2_JcHzIcez_",
        "colab_type": "text"
      },
      "source": [
        "# Walktrough Flair Framework using Conll_03 as dataset + different embeddings\n",
        "\n",
        "## Embeddings:\n",
        "\n",
        "\n",
        "*   FastText\n",
        "*   BytePair\n",
        "*   Flair\n",
        "*   PooledFlair\n",
        "*   BERT base, Multilingual, German (deepset.ai), GERMAN (dbmdz)\n",
        "*   Hot One\n",
        "*   XLM German+English (CLM/-)\n",
        "*   DistilBERT German, Multilingual\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcyMfa4Hk9L2",
        "colab_type": "text"
      },
      "source": [
        "## 1. Install and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQrkdWR8OPdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install flair\n",
        "pip install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2cEcB-qZwHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see if flair is installed\n",
        "!pip show flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8dPj23vkHTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import CONLL_03_GERMAN, ColumnCorpus, GERMEVAL \n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings, OneHotEmbeddings, BertEmbeddings, XLMEmbeddings, BytePairEmbeddings\n",
        "from flair.visual.training_curves import Plotter\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.models import SequenceTagger\n",
        "from typing import List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWfGv7h2Gh-W",
        "colab_type": "text"
      },
      "source": [
        "## 2. Set the corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXqWPb5aLd9J",
        "colab_type": "text"
      },
      "source": [
        "### Conll_03\n",
        "#### Formatted in BIOES-Format\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ2-mfwKagw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = {0: 'text', 1: 'ner'}\n",
        "data_folder = 'resources/tasks/conll_03_german'\n",
        "\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='deuBIOES.train',\n",
        "                              test_file='deuBIOES.testa',\n",
        "                              dev_file='deuBIOES.testb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZacepc8LhqZ",
        "colab_type": "text"
      },
      "source": [
        "### GermEval_14\n",
        "#### Formatted in BIOES-Format\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "349o_xeyGr0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = {0: 'text', 1: 'ner'}\n",
        "data_folder = 'resources/tasks/germeval'\n",
        "\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='trainBIOES.train',\n",
        "                              test_file='testBIOES.test',\n",
        "                              dev_file='devBIOES.dev')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF4NtcQlLplO",
        "colab_type": "text"
      },
      "source": [
        "### Own Corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBnv-jIpLrgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = {0: 'text', 1: 'ner'}\n",
        "data_folder = '/content/drive/My Drive/resources/tasks/conll_03_german'\n",
        "\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='deuBIOES.train',\n",
        "                              test_file='deuBIOES.testa',\n",
        "                              dev_file='deuBIOES.testb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI2_i56SxubP",
        "colab_type": "text"
      },
      "source": [
        "### Define Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fspFpCHzb4H_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define task + tag dict.\n",
        "tag_type = 'ner'\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "print(tag_dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAd8EkjOUeH8",
        "colab_type": "text"
      },
      "source": [
        "## 3. Set Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUYn8Bs3X2rK",
        "colab_type": "text"
      },
      "source": [
        "### Word (fastext) + BytePairEmbeddings Embeddings\n",
        "#### BytePair for oov-functionality \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj88rDnuYmba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "                                         BytePairEmbeddings('de')                                                 \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5z0MJFDY72k",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext over crawl) + BytePairEmbeddings Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6_I18kLZA8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de-crawl'),\n",
        "                                         BytePairEmbeddings('de')                                                   \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJoKW37iUsyY",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + Flair Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L43bPcondLAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         FlairEmbeddings('de-forward'),\n",
        "                                         FlairEmbeddings('de-backward'),                                                   \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgqgjdvlgM-q",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + Flair (de-history) Embeddings \n",
        "#### Hamburger Anzeiger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpRliKT3gUoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         FlairEmbeddings('de-historic-ha-forward'),\n",
        "                                         FlairEmbeddings('de-historic-ha-backward'),                                                   \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jEmXUeRogl92"
      },
      "source": [
        "### Word (fasttext) + Flair (de-history) Embeddings \n",
        "#### Wiener Zeitung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HqqTOHTsgl97",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         FlairEmbeddings('de-historic-wz-forward'),\n",
        "                                         FlairEmbeddings('de-historic-wz-backward'),                                                   \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDblEo_YZ2hd",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + PooledFlair Embeddings\n",
        "#### Best known configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp24kaCSZ_ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         PooledFlairEmbeddings('german-forward'),\n",
        "                                         PooledFlairEmbeddings('german-backward'),                                                   \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUp-34p9aqz5",
        "colab_type": "text"
      },
      "source": [
        "### PooledFlair Embeddings + dbmdz-BERT-german-cased Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8YpWXpa0Pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         PooledFlairEmbeddings('german-forward'),\n",
        "                                         PooledFlairEmbeddings('german-backward'),\n",
        "                                          \n",
        "                                         BertEmbeddings('bert-base-german-dbmdz-cased')\n",
        "                                                                                             \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4iWznPwUxO0",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + dbmdz-BERT-german-cased Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S50SDIfAU02X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         BertEmbeddings('bert-base-german-dbmdz-cased'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6FnOfvlQbpTb"
      },
      "source": [
        "### Word (fasttext) + dbmdz-BERT-german-uncased Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KOl8Ry4UbpTf",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         BertEmbeddings('bert-base-german-dbmdz-uncased'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEkCXfLrZIz8",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + BERT-base-cased Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQvJ3EDtZOc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         BertEmbeddings('bert-base-cased'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aHFumxgxbvAF"
      },
      "source": [
        "### Word (fasttext) + BERT-base-uncased Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JU2hESKIbvAI",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         BertEmbeddings('bert-base-uncased'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oTyHf5jZblf",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + BERT-multilingual-cased Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duMnUjUBZk02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         BertEmbeddings('bert-base-multilingual-cased'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sMxCCGWWb772"
      },
      "source": [
        "### Word (fasttext) + BERT-multilingual-uncased Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ghKuAhb7b776",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         BertEmbeddings('bert-base-multilingual-uncased'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEinDdOReGtm",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + BERT-base-german-cased\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNmgr74GeL8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         BertEmbeddings('bert-base-german-cased'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i281Wbd5ejlG",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + distilbert-base-german-cased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d42UlqrIenHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         BertEmbeddings('distilbert-base-german-cased'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4icwuEtesow",
        "colab_type": "text"
      },
      "source": [
        "### Word (fastext) + distilbert-base-multilingual-cased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3raDCpPewPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         BertEmbeddings('distilbert-base-multilingual-cased'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urbljo5SiHLg",
        "colab_type": "text"
      },
      "source": [
        "### One Hot Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPOtsglkiK11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load corpus\n",
        "corpus = \"Own Corpus\"\n",
        "\n",
        "# embed NER tags\n",
        "embeddings = OneHotEmbeddings(corpus=corpus, field='ner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veiZOL0ijsD9",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + XLM (german + english)\n",
        "#### XLM English-German model trained on the concatenation of English and German wikipedia < 6-layer, 1024-hidden, 8-heads >\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxoMCAx8j5qO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         XLMEmbeddings('xlm-mlm-ende-1024'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijJcsDyDj6I2",
        "colab_type": "text"
      },
      "source": [
        "### Word (fasttext) + XLM (german + english)\n",
        "#### XLM English-German model trained with CLM (Causal Language Modeling) on the concatenation of English and German wikipedia < 6-layer, 1024-hidden, 8-heads >\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR7k27cyj--r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize embeddings \n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "                                         WordEmbeddings('de'),\n",
        "\n",
        "                                         XLMEmbeddings('xlm-clm-ende-1024'),\n",
        "                                                                                            \n",
        "]\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6g5KjFIbQdD",
        "colab_type": "text"
      },
      "source": [
        "## 4. Set the remaining params and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Eqf02nBgSOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize sequence tagger\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPqPcqgugcJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize trainer\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCs4NAeKg3aU",
        "colab_type": "code",
        "outputId": "c7a75d8a-795a-4c64-c885-857fea88d6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# start training \n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              max_epochs=40,\n",
        "              train_with_dev='True',\n",
        "              checkpoint=True, \n",
        "              embeddings_storage_mode='gpu')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-17 11:12:33,145 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 11:12:33,150 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings('de')\n",
            "    (list_embedding_1): BertEmbeddings(\n",
            "      (model): BertModel(\n",
            "        (embeddings): BertEmbeddings(\n",
            "          (word_embeddings): Embedding(31102, 768, padding_idx=0)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): BertEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (1): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (2): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (3): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (4): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (5): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (6): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (7): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (8): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (9): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (10): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (11): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (pooler): BertPooler(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): Tanh()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=3372, out_features=3372, bias=True)\n",
            "  (rnn): LSTM(3372, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-04-17 11:12:33,152 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 11:12:33,154 Corpus: \"Corpus: 12705 train + 3160 dev + 3068 test sentences\"\n",
            "2020-04-17 11:12:33,155 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 11:12:33,156 Parameters:\n",
            "2020-04-17 11:12:33,158  - learning_rate: \"0.1\"\n",
            "2020-04-17 11:12:33,160  - mini_batch_size: \"32\"\n",
            "2020-04-17 11:12:33,161  - patience: \"3\"\n",
            "2020-04-17 11:12:33,163  - anneal_factor: \"0.5\"\n",
            "2020-04-17 11:12:33,164  - max_epochs: \"40\"\n",
            "2020-04-17 11:12:33,165  - shuffle: \"True\"\n",
            "2020-04-17 11:12:33,165  - train_with_dev: \"True\"\n",
            "2020-04-17 11:12:33,167  - batch_growth_annealing: \"False\"\n",
            "2020-04-17 11:12:33,167 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 11:12:33,168 Model training base path: \"resources/taggers/example-ner\"\n",
            "2020-04-17 11:12:33,169 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 11:12:33,170 Device: cpu\n",
            "2020-04-17 11:12:33,171 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 11:12:33,172 Embeddings storage mode: gpu\n",
            "2020-04-17 11:12:33,176 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 11:19:44,294 epoch 1 - iter 49/496 - loss 10.75793314 - samples/sec: 3.64\n",
            "2020-04-17 11:26:43,782 epoch 1 - iter 98/496 - loss 8.42458364 - samples/sec: 3.74\n",
            "2020-04-17 11:33:52,956 epoch 1 - iter 147/496 - loss 7.25647406 - samples/sec: 3.65\n",
            "2020-04-17 11:41:12,560 epoch 1 - iter 196/496 - loss 6.45446427 - samples/sec: 3.57\n",
            "2020-04-17 11:48:08,506 epoch 1 - iter 245/496 - loss 5.86068336 - samples/sec: 3.77\n",
            "2020-04-17 11:55:06,682 epoch 1 - iter 294/496 - loss 5.40360927 - samples/sec: 3.75\n",
            "2020-04-17 12:02:14,189 epoch 1 - iter 343/496 - loss 5.05953013 - samples/sec: 3.67\n",
            "2020-04-17 12:08:54,568 epoch 1 - iter 392/496 - loss 4.76662240 - samples/sec: 3.92\n",
            "2020-04-17 12:15:55,321 epoch 1 - iter 441/496 - loss 4.52447023 - samples/sec: 3.73\n",
            "2020-04-17 12:22:44,737 epoch 1 - iter 490/496 - loss 4.32352501 - samples/sec: 3.83\n",
            "2020-04-17 12:23:27,439 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 12:23:27,440 EPOCH 1 done: loss 4.3020 - lr 0.1000\n",
            "2020-04-17 12:23:27,444 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 12:23:27,445 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 12:25:07,465 epoch 2 - iter 49/496 - loss 2.30624963 - samples/sec: 15.68\n",
            "2020-04-17 12:26:48,163 epoch 2 - iter 98/496 - loss 2.33557042 - samples/sec: 15.58\n",
            "2020-04-17 12:28:25,631 epoch 2 - iter 147/496 - loss 2.33713072 - samples/sec: 16.10\n",
            "2020-04-17 12:30:08,989 epoch 2 - iter 196/496 - loss 2.29305450 - samples/sec: 15.18\n",
            "2020-04-17 12:31:48,948 epoch 2 - iter 245/496 - loss 2.24937602 - samples/sec: 15.70\n",
            "2020-04-17 12:33:26,527 epoch 2 - iter 294/496 - loss 2.20945554 - samples/sec: 16.08\n",
            "2020-04-17 12:35:04,324 epoch 2 - iter 343/496 - loss 2.17201081 - samples/sec: 16.05\n",
            "2020-04-17 12:36:43,076 epoch 2 - iter 392/496 - loss 2.12952173 - samples/sec: 15.89\n",
            "2020-04-17 12:38:24,904 epoch 2 - iter 441/496 - loss 2.12023677 - samples/sec: 15.41\n",
            "2020-04-17 12:40:08,710 epoch 2 - iter 490/496 - loss 2.08875158 - samples/sec: 15.12\n",
            "2020-04-17 12:40:19,947 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 12:40:19,948 EPOCH 2 done: loss 2.0858 - lr 0.1000\n",
            "2020-04-17 12:40:19,953 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 12:40:19,956 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 12:41:55,474 epoch 3 - iter 49/496 - loss 1.72544050 - samples/sec: 16.42\n",
            "2020-04-17 12:43:35,752 epoch 3 - iter 98/496 - loss 1.76010038 - samples/sec: 15.65\n",
            "2020-04-17 12:45:15,729 epoch 3 - iter 147/496 - loss 1.76931757 - samples/sec: 15.70\n",
            "2020-04-17 12:46:55,501 epoch 3 - iter 196/496 - loss 1.74006187 - samples/sec: 15.73\n",
            "2020-04-17 12:48:34,444 epoch 3 - iter 245/496 - loss 1.75727796 - samples/sec: 15.86\n",
            "2020-04-17 12:50:16,048 epoch 3 - iter 294/496 - loss 1.77783641 - samples/sec: 15.44\n",
            "2020-04-17 12:51:54,310 epoch 3 - iter 343/496 - loss 1.74456706 - samples/sec: 15.97\n",
            "2020-04-17 12:53:34,919 epoch 3 - iter 392/496 - loss 1.70725870 - samples/sec: 15.60\n",
            "2020-04-17 12:55:15,590 epoch 3 - iter 441/496 - loss 1.69891326 - samples/sec: 15.59\n",
            "2020-04-17 12:56:51,935 epoch 3 - iter 490/496 - loss 1.67643298 - samples/sec: 16.29\n",
            "2020-04-17 12:57:02,915 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 12:57:02,916 EPOCH 3 done: loss 1.6737 - lr 0.1000\n",
            "2020-04-17 12:57:02,920 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 12:57:02,922 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 12:58:40,068 epoch 4 - iter 49/496 - loss 1.44578903 - samples/sec: 16.14\n",
            "2020-04-17 13:00:21,224 epoch 4 - iter 98/496 - loss 1.50494413 - samples/sec: 15.51\n",
            "2020-04-17 13:01:58,616 epoch 4 - iter 147/496 - loss 1.51797000 - samples/sec: 16.11\n",
            "2020-04-17 13:03:38,748 epoch 4 - iter 196/496 - loss 1.52864213 - samples/sec: 15.67\n",
            "2020-04-17 13:05:23,166 epoch 4 - iter 245/496 - loss 1.52047026 - samples/sec: 15.03\n",
            "2020-04-17 13:07:07,225 epoch 4 - iter 294/496 - loss 1.55857519 - samples/sec: 15.08\n",
            "2020-04-17 13:08:46,509 epoch 4 - iter 343/496 - loss 1.54232322 - samples/sec: 15.81\n",
            "2020-04-17 13:10:26,304 epoch 4 - iter 392/496 - loss 1.53460754 - samples/sec: 15.73\n",
            "2020-04-17 13:12:09,852 epoch 4 - iter 441/496 - loss 1.52072039 - samples/sec: 15.15\n",
            "2020-04-17 13:13:51,360 epoch 4 - iter 490/496 - loss 1.51117802 - samples/sec: 15.46\n",
            "2020-04-17 13:14:03,274 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 13:14:03,275 EPOCH 4 done: loss 1.5089 - lr 0.1000\n",
            "2020-04-17 13:14:03,279 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 13:14:03,281 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 13:15:43,636 epoch 5 - iter 49/496 - loss 1.42397941 - samples/sec: 15.63\n",
            "2020-04-17 13:17:26,391 epoch 5 - iter 98/496 - loss 1.40760962 - samples/sec: 15.27\n",
            "2020-04-17 13:19:06,370 epoch 5 - iter 147/496 - loss 1.37581279 - samples/sec: 15.70\n",
            "2020-04-17 13:20:49,702 epoch 5 - iter 196/496 - loss 1.40264006 - samples/sec: 15.19\n",
            "2020-04-17 13:22:27,437 epoch 5 - iter 245/496 - loss 1.36995200 - samples/sec: 16.06\n",
            "2020-04-17 13:24:14,658 epoch 5 - iter 294/496 - loss 1.37627272 - samples/sec: 14.64\n",
            "2020-04-17 13:26:08,603 epoch 5 - iter 343/496 - loss 1.37844315 - samples/sec: 13.77\n",
            "2020-04-17 13:27:50,826 epoch 5 - iter 392/496 - loss 1.36960651 - samples/sec: 15.35\n",
            "2020-04-17 13:29:34,020 epoch 5 - iter 441/496 - loss 1.37295347 - samples/sec: 15.21\n",
            "2020-04-17 13:31:13,686 epoch 5 - iter 490/496 - loss 1.37596591 - samples/sec: 15.75\n",
            "2020-04-17 13:31:26,095 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 13:31:26,096 EPOCH 5 done: loss 1.3740 - lr 0.1000\n",
            "2020-04-17 13:31:26,097 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 13:31:26,098 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 13:33:10,455 epoch 6 - iter 49/496 - loss 1.43134649 - samples/sec: 15.03\n",
            "2020-04-17 13:34:50,394 epoch 6 - iter 98/496 - loss 1.34681254 - samples/sec: 15.70\n",
            "2020-04-17 13:36:31,338 epoch 6 - iter 147/496 - loss 1.32877874 - samples/sec: 15.55\n",
            "2020-04-17 13:38:17,353 epoch 6 - iter 196/496 - loss 1.32686466 - samples/sec: 14.80\n",
            "2020-04-17 13:40:03,627 epoch 6 - iter 245/496 - loss 1.31770969 - samples/sec: 14.77\n",
            "2020-04-17 13:41:46,338 epoch 6 - iter 294/496 - loss 1.30629360 - samples/sec: 15.28\n",
            "2020-04-17 13:43:25,698 epoch 6 - iter 343/496 - loss 1.30859405 - samples/sec: 15.79\n",
            "2020-04-17 13:45:10,561 epoch 6 - iter 392/496 - loss 1.31630106 - samples/sec: 14.96\n",
            "2020-04-17 13:46:48,882 epoch 6 - iter 441/496 - loss 1.29848166 - samples/sec: 15.96\n",
            "2020-04-17 13:48:31,852 epoch 6 - iter 490/496 - loss 1.28731369 - samples/sec: 15.24\n",
            "2020-04-17 13:48:43,880 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 13:48:43,881 EPOCH 6 done: loss 1.2866 - lr 0.1000\n",
            "2020-04-17 13:48:43,882 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 13:48:43,884 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 13:50:25,901 epoch 7 - iter 49/496 - loss 1.14093362 - samples/sec: 15.37\n",
            "2020-04-17 13:52:06,682 epoch 7 - iter 98/496 - loss 1.19570348 - samples/sec: 15.57\n",
            "2020-04-17 13:53:52,642 epoch 7 - iter 147/496 - loss 1.20125610 - samples/sec: 14.81\n",
            "2020-04-17 13:55:30,558 epoch 7 - iter 196/496 - loss 1.19613910 - samples/sec: 16.03\n",
            "2020-04-17 13:57:11,900 epoch 7 - iter 245/496 - loss 1.18853393 - samples/sec: 15.49\n",
            "2020-04-17 13:58:52,565 epoch 7 - iter 294/496 - loss 1.18893647 - samples/sec: 15.59\n",
            "2020-04-17 14:00:32,386 epoch 7 - iter 343/496 - loss 1.18509229 - samples/sec: 15.72\n",
            "2020-04-17 14:02:14,892 epoch 7 - iter 392/496 - loss 1.20159717 - samples/sec: 15.31\n",
            "2020-04-17 14:03:53,254 epoch 7 - iter 441/496 - loss 1.20638123 - samples/sec: 15.95\n",
            "2020-04-17 14:05:35,826 epoch 7 - iter 490/496 - loss 1.21533431 - samples/sec: 15.30\n",
            "2020-04-17 14:05:48,814 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 14:05:48,815 EPOCH 7 done: loss 1.2153 - lr 0.1000\n",
            "2020-04-17 14:05:48,817 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 14:05:48,820 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 14:07:30,454 epoch 8 - iter 49/496 - loss 1.21310208 - samples/sec: 15.43\n",
            "2020-04-17 14:09:11,828 epoch 8 - iter 98/496 - loss 1.24874372 - samples/sec: 15.48\n",
            "2020-04-17 14:10:55,440 epoch 8 - iter 147/496 - loss 1.23947769 - samples/sec: 15.15\n",
            "2020-04-17 14:12:38,436 epoch 8 - iter 196/496 - loss 1.22411525 - samples/sec: 15.24\n",
            "2020-04-17 14:14:22,850 epoch 8 - iter 245/496 - loss 1.21945426 - samples/sec: 15.03\n",
            "2020-04-17 14:16:06,630 epoch 8 - iter 294/496 - loss 1.20399329 - samples/sec: 15.12\n",
            "2020-04-17 14:17:42,446 epoch 8 - iter 343/496 - loss 1.18747782 - samples/sec: 16.38\n",
            "2020-04-17 14:19:24,534 epoch 8 - iter 392/496 - loss 1.19042181 - samples/sec: 15.37\n",
            "2020-04-17 14:21:00,144 epoch 8 - iter 441/496 - loss 1.18456910 - samples/sec: 16.41\n",
            "2020-04-17 14:22:39,681 epoch 8 - iter 490/496 - loss 1.17600598 - samples/sec: 15.77\n",
            "2020-04-17 14:22:51,310 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 14:22:51,311 EPOCH 8 done: loss 1.1760 - lr 0.1000\n",
            "2020-04-17 14:22:51,312 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 14:22:51,318 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 14:24:29,857 epoch 9 - iter 49/496 - loss 1.04467953 - samples/sec: 15.91\n",
            "2020-04-17 14:26:09,626 epoch 9 - iter 98/496 - loss 1.04538726 - samples/sec: 15.73\n",
            "2020-04-17 14:27:52,069 epoch 9 - iter 147/496 - loss 1.07483371 - samples/sec: 15.32\n",
            "2020-04-17 14:29:34,488 epoch 9 - iter 196/496 - loss 1.10727711 - samples/sec: 15.32\n",
            "2020-04-17 14:31:18,123 epoch 9 - iter 245/496 - loss 1.10267477 - samples/sec: 15.14\n",
            "2020-04-17 14:33:00,020 epoch 9 - iter 294/496 - loss 1.11971766 - samples/sec: 15.40\n",
            "2020-04-17 14:34:36,242 epoch 9 - iter 343/496 - loss 1.10584364 - samples/sec: 16.31\n",
            "2020-04-17 14:36:16,806 epoch 9 - iter 392/496 - loss 1.11251355 - samples/sec: 15.60\n",
            "2020-04-17 14:37:59,931 epoch 9 - iter 441/496 - loss 1.11493028 - samples/sec: 15.22\n",
            "2020-04-17 14:39:37,432 epoch 9 - iter 490/496 - loss 1.11074976 - samples/sec: 16.09\n",
            "2020-04-17 14:39:49,426 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 14:39:49,427 EPOCH 9 done: loss 1.1122 - lr 0.1000\n",
            "2020-04-17 14:39:49,428 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 14:39:49,430 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 14:41:27,072 epoch 10 - iter 49/496 - loss 1.01943824 - samples/sec: 16.06\n",
            "2020-04-17 14:43:07,605 epoch 10 - iter 98/496 - loss 1.07092144 - samples/sec: 15.61\n",
            "2020-04-17 14:44:46,985 epoch 10 - iter 147/496 - loss 1.12144407 - samples/sec: 15.79\n",
            "2020-04-17 14:46:27,872 epoch 10 - iter 196/496 - loss 1.10560403 - samples/sec: 15.55\n",
            "2020-04-17 14:48:09,222 epoch 10 - iter 245/496 - loss 1.10036999 - samples/sec: 15.48\n",
            "2020-04-17 14:49:51,066 epoch 10 - iter 294/496 - loss 1.11148293 - samples/sec: 15.41\n",
            "2020-04-17 14:51:34,563 epoch 10 - iter 343/496 - loss 1.09644620 - samples/sec: 15.16\n",
            "2020-04-17 14:53:14,515 epoch 10 - iter 392/496 - loss 1.09530062 - samples/sec: 15.70\n",
            "2020-04-17 14:54:54,730 epoch 10 - iter 441/496 - loss 1.09102658 - samples/sec: 15.66\n",
            "2020-04-17 14:56:36,917 epoch 10 - iter 490/496 - loss 1.09333807 - samples/sec: 15.36\n",
            "2020-04-17 14:56:49,253 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 14:56:49,254 EPOCH 10 done: loss 1.0953 - lr 0.1000\n",
            "2020-04-17 14:56:49,256 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 14:56:49,260 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 14:58:27,965 epoch 11 - iter 49/496 - loss 1.01140248 - samples/sec: 15.89\n",
            "2020-04-17 15:00:06,270 epoch 11 - iter 98/496 - loss 1.04095994 - samples/sec: 15.96\n",
            "2020-04-17 15:01:47,265 epoch 11 - iter 147/496 - loss 1.04336168 - samples/sec: 15.54\n",
            "2020-04-17 15:03:27,341 epoch 11 - iter 196/496 - loss 1.03127047 - samples/sec: 15.68\n",
            "2020-04-17 15:05:09,873 epoch 11 - iter 245/496 - loss 1.03908659 - samples/sec: 15.31\n",
            "2020-04-17 15:06:50,808 epoch 11 - iter 294/496 - loss 1.03888862 - samples/sec: 15.55\n",
            "2020-04-17 15:08:32,987 epoch 11 - iter 343/496 - loss 1.04955194 - samples/sec: 15.36\n",
            "2020-04-17 15:10:16,033 epoch 11 - iter 392/496 - loss 1.03905452 - samples/sec: 15.23\n",
            "2020-04-17 15:12:00,045 epoch 11 - iter 441/496 - loss 1.04832728 - samples/sec: 15.09\n",
            "2020-04-17 15:13:45,040 epoch 11 - iter 490/496 - loss 1.04058554 - samples/sec: 14.95\n",
            "2020-04-17 15:13:56,827 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 15:13:56,828 EPOCH 11 done: loss 1.0374 - lr 0.1000\n",
            "2020-04-17 15:13:56,832 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 15:13:56,833 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 15:15:41,226 epoch 12 - iter 49/496 - loss 1.10269722 - samples/sec: 15.02\n",
            "2020-04-17 15:17:21,594 epoch 12 - iter 98/496 - loss 1.08949119 - samples/sec: 15.63\n",
            "2020-04-17 15:19:08,521 epoch 12 - iter 147/496 - loss 1.06817501 - samples/sec: 14.68\n",
            "2020-04-17 15:20:53,930 epoch 12 - iter 196/496 - loss 1.05905247 - samples/sec: 14.89\n",
            "2020-04-17 15:22:36,288 epoch 12 - iter 245/496 - loss 1.04423548 - samples/sec: 15.33\n",
            "2020-04-17 15:24:21,934 epoch 12 - iter 294/496 - loss 1.04799750 - samples/sec: 14.85\n",
            "2020-04-17 15:26:05,359 epoch 12 - iter 343/496 - loss 1.02793663 - samples/sec: 15.17\n",
            "2020-04-17 15:27:43,446 epoch 12 - iter 392/496 - loss 1.02216467 - samples/sec: 16.00\n",
            "2020-04-17 15:29:26,193 epoch 12 - iter 441/496 - loss 1.01818071 - samples/sec: 15.27\n",
            "2020-04-17 15:31:07,025 epoch 12 - iter 490/496 - loss 1.01214511 - samples/sec: 15.56\n",
            "2020-04-17 15:31:19,073 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 15:31:19,074 EPOCH 12 done: loss 1.0096 - lr 0.1000\n",
            "2020-04-17 15:31:19,078 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 15:31:19,080 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 15:33:01,106 epoch 13 - iter 49/496 - loss 1.00242505 - samples/sec: 15.37\n",
            "2020-04-17 15:34:45,756 epoch 13 - iter 98/496 - loss 0.97267337 - samples/sec: 15.00\n",
            "2020-04-17 15:36:33,555 epoch 13 - iter 147/496 - loss 0.99919135 - samples/sec: 14.56\n",
            "2020-04-17 15:38:20,928 epoch 13 - iter 196/496 - loss 1.00343745 - samples/sec: 14.61\n",
            "2020-04-17 15:40:01,831 epoch 13 - iter 245/496 - loss 1.00889202 - samples/sec: 15.55\n",
            "2020-04-17 15:41:43,440 epoch 13 - iter 294/496 - loss 0.98900464 - samples/sec: 15.44\n",
            "2020-04-17 15:43:26,116 epoch 13 - iter 343/496 - loss 0.99354224 - samples/sec: 15.28\n",
            "2020-04-17 15:45:11,191 epoch 13 - iter 392/496 - loss 0.99414958 - samples/sec: 14.93\n",
            "2020-04-17 15:46:57,312 epoch 13 - iter 441/496 - loss 0.98263687 - samples/sec: 14.79\n",
            "2020-04-17 15:48:44,304 epoch 13 - iter 490/496 - loss 0.97754399 - samples/sec: 14.67\n",
            "2020-04-17 15:48:56,101 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 15:48:56,102 EPOCH 13 done: loss 0.9782 - lr 0.1000\n",
            "2020-04-17 15:48:56,104 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 15:48:56,106 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 15:50:43,659 epoch 14 - iter 49/496 - loss 1.09079409 - samples/sec: 14.58\n",
            "2020-04-17 15:52:25,450 epoch 14 - iter 98/496 - loss 0.99541204 - samples/sec: 15.42\n",
            "2020-04-17 15:54:10,406 epoch 14 - iter 147/496 - loss 0.98738222 - samples/sec: 14.95\n",
            "2020-04-17 15:55:57,195 epoch 14 - iter 196/496 - loss 1.00093541 - samples/sec: 14.70\n",
            "2020-04-17 15:57:44,270 epoch 14 - iter 245/496 - loss 0.98786546 - samples/sec: 14.66\n",
            "2020-04-17 15:59:25,157 epoch 14 - iter 294/496 - loss 0.97563987 - samples/sec: 15.56\n",
            "2020-04-17 16:01:12,007 epoch 14 - iter 343/496 - loss 0.98154651 - samples/sec: 14.69\n",
            "2020-04-17 16:02:58,196 epoch 14 - iter 392/496 - loss 0.97719166 - samples/sec: 14.78\n",
            "2020-04-17 16:04:44,478 epoch 14 - iter 441/496 - loss 0.96870807 - samples/sec: 14.77\n",
            "2020-04-17 16:06:32,386 epoch 14 - iter 490/496 - loss 0.95697072 - samples/sec: 14.54\n",
            "2020-04-17 16:06:45,228 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 16:06:45,229 EPOCH 14 done: loss 0.9538 - lr 0.1000\n",
            "2020-04-17 16:06:45,234 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 16:06:45,236 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 16:08:32,034 epoch 15 - iter 49/496 - loss 1.00047141 - samples/sec: 14.68\n",
            "2020-04-17 16:10:17,998 epoch 15 - iter 98/496 - loss 1.02851363 - samples/sec: 14.81\n",
            "2020-04-17 16:12:03,190 epoch 15 - iter 147/496 - loss 1.00785873 - samples/sec: 14.92\n",
            "2020-04-17 16:13:52,664 epoch 15 - iter 196/496 - loss 0.98646344 - samples/sec: 14.33\n",
            "2020-04-17 16:15:41,086 epoch 15 - iter 245/496 - loss 0.98200963 - samples/sec: 14.47\n",
            "2020-04-17 16:17:32,247 epoch 15 - iter 294/496 - loss 0.97228070 - samples/sec: 14.12\n",
            "2020-04-17 16:19:21,179 epoch 15 - iter 343/496 - loss 0.96663711 - samples/sec: 14.41\n",
            "2020-04-17 16:21:04,307 epoch 15 - iter 392/496 - loss 0.96188692 - samples/sec: 15.22\n",
            "2020-04-17 16:22:47,192 epoch 15 - iter 441/496 - loss 0.95501584 - samples/sec: 15.25\n",
            "2020-04-17 16:24:37,750 epoch 15 - iter 490/496 - loss 0.95273565 - samples/sec: 14.19\n",
            "2020-04-17 16:24:50,432 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 16:24:50,433 EPOCH 15 done: loss 0.9515 - lr 0.1000\n",
            "2020-04-17 16:24:50,434 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 16:24:50,439 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 16:26:37,407 epoch 16 - iter 49/496 - loss 1.01048920 - samples/sec: 14.66\n",
            "2020-04-17 16:28:25,632 epoch 16 - iter 98/496 - loss 0.96551753 - samples/sec: 14.50\n",
            "2020-04-17 16:30:11,802 epoch 16 - iter 147/496 - loss 0.94985227 - samples/sec: 14.78\n",
            "2020-04-17 16:31:56,974 epoch 16 - iter 196/496 - loss 0.93619071 - samples/sec: 14.92\n",
            "2020-04-17 16:33:46,278 epoch 16 - iter 245/496 - loss 0.93122103 - samples/sec: 14.36\n",
            "2020-04-17 16:35:32,918 epoch 16 - iter 294/496 - loss 0.91714168 - samples/sec: 14.71\n",
            "2020-04-17 16:37:23,208 epoch 16 - iter 343/496 - loss 0.91601363 - samples/sec: 14.23\n",
            "2020-04-17 16:39:07,443 epoch 16 - iter 392/496 - loss 0.92640354 - samples/sec: 15.06\n",
            "2020-04-17 16:40:55,198 epoch 16 - iter 441/496 - loss 0.92103346 - samples/sec: 14.56\n",
            "2020-04-17 16:42:48,686 epoch 16 - iter 490/496 - loss 0.92069459 - samples/sec: 13.83\n",
            "2020-04-17 16:43:01,111 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 16:43:01,112 EPOCH 16 done: loss 0.9248 - lr 0.1000\n",
            "2020-04-17 16:43:01,114 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 16:43:01,116 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 16:44:53,540 epoch 17 - iter 49/496 - loss 0.92468714 - samples/sec: 13.95\n",
            "2020-04-17 16:46:42,467 epoch 17 - iter 98/496 - loss 0.92966628 - samples/sec: 14.41\n",
            "2020-04-17 16:48:31,497 epoch 17 - iter 147/496 - loss 0.91375871 - samples/sec: 14.39\n",
            "2020-04-17 16:50:25,377 epoch 17 - iter 196/496 - loss 0.92473868 - samples/sec: 13.78\n",
            "2020-04-17 16:52:15,337 epoch 17 - iter 245/496 - loss 0.92017783 - samples/sec: 14.27\n",
            "2020-04-17 16:54:07,303 epoch 17 - iter 294/496 - loss 0.93236595 - samples/sec: 14.01\n",
            "2020-04-17 16:55:55,741 epoch 17 - iter 343/496 - loss 0.92701044 - samples/sec: 14.47\n",
            "2020-04-17 16:57:47,517 epoch 17 - iter 392/496 - loss 0.91710619 - samples/sec: 14.04\n",
            "2020-04-17 16:59:38,869 epoch 17 - iter 441/496 - loss 0.91277169 - samples/sec: 14.09\n",
            "2020-04-17 17:01:26,842 epoch 17 - iter 490/496 - loss 0.91181292 - samples/sec: 14.53\n",
            "2020-04-17 17:01:39,535 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 17:01:39,536 EPOCH 17 done: loss 0.9119 - lr 0.1000\n",
            "2020-04-17 17:01:39,538 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 17:01:39,540 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 17:03:29,490 epoch 18 - iter 49/496 - loss 0.81071115 - samples/sec: 14.26\n",
            "2020-04-17 17:05:16,689 epoch 18 - iter 98/496 - loss 0.84959511 - samples/sec: 14.64\n",
            "2020-04-17 17:07:05,629 epoch 18 - iter 147/496 - loss 0.84417430 - samples/sec: 14.40\n",
            "2020-04-17 17:08:56,076 epoch 18 - iter 196/496 - loss 0.86285153 - samples/sec: 14.21\n",
            "2020-04-17 17:10:50,154 epoch 18 - iter 245/496 - loss 0.86856901 - samples/sec: 13.76\n",
            "2020-04-17 17:12:39,388 epoch 18 - iter 294/496 - loss 0.87504867 - samples/sec: 14.37\n",
            "2020-04-17 17:14:27,552 epoch 18 - iter 343/496 - loss 0.88507704 - samples/sec: 14.51\n",
            "2020-04-17 17:16:16,156 epoch 18 - iter 392/496 - loss 0.89269585 - samples/sec: 14.45\n",
            "2020-04-17 17:18:09,035 epoch 18 - iter 441/496 - loss 0.89953336 - samples/sec: 13.90\n",
            "2020-04-17 17:19:56,114 epoch 18 - iter 490/496 - loss 0.90113979 - samples/sec: 14.65\n",
            "2020-04-17 17:20:08,167 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 17:20:08,168 EPOCH 18 done: loss 0.8983 - lr 0.1000\n",
            "2020-04-17 17:20:08,173 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 17:20:08,175 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 17:21:52,545 epoch 19 - iter 49/496 - loss 0.87949802 - samples/sec: 15.02\n",
            "2020-04-17 17:23:38,299 epoch 19 - iter 98/496 - loss 0.86586561 - samples/sec: 14.84\n",
            "2020-04-17 17:25:30,140 epoch 19 - iter 147/496 - loss 0.89357444 - samples/sec: 14.03\n",
            "2020-04-17 17:27:21,804 epoch 19 - iter 196/496 - loss 0.90637890 - samples/sec: 14.05\n",
            "2020-04-17 17:29:12,489 epoch 19 - iter 245/496 - loss 0.90956134 - samples/sec: 14.18\n",
            "2020-04-17 17:31:03,018 epoch 19 - iter 294/496 - loss 0.91496473 - samples/sec: 14.20\n",
            "2020-04-17 17:32:53,794 epoch 19 - iter 343/496 - loss 0.91214041 - samples/sec: 14.17\n",
            "2020-04-17 17:34:43,764 epoch 19 - iter 392/496 - loss 0.90536451 - samples/sec: 14.27\n",
            "2020-04-17 17:36:38,967 epoch 19 - iter 441/496 - loss 0.90338943 - samples/sec: 13.62\n",
            "2020-04-17 17:38:24,926 epoch 19 - iter 490/496 - loss 0.90050428 - samples/sec: 14.81\n",
            "2020-04-17 17:38:38,322 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 17:38:38,323 EPOCH 19 done: loss 0.8973 - lr 0.1000\n",
            "2020-04-17 17:38:38,327 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 17:38:38,329 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 17:40:23,194 epoch 20 - iter 49/496 - loss 0.84016944 - samples/sec: 14.95\n",
            "2020-04-17 17:42:14,759 epoch 20 - iter 98/496 - loss 0.89612809 - samples/sec: 14.06\n",
            "2020-04-17 17:44:09,098 epoch 20 - iter 147/496 - loss 0.88101395 - samples/sec: 13.72\n",
            "2020-04-17 17:46:01,002 epoch 20 - iter 196/496 - loss 0.85690343 - samples/sec: 14.02\n",
            "2020-04-17 17:47:56,126 epoch 20 - iter 245/496 - loss 0.86169112 - samples/sec: 13.63\n",
            "2020-04-17 17:49:45,054 epoch 20 - iter 294/496 - loss 0.87006338 - samples/sec: 14.41\n",
            "2020-04-17 17:51:41,147 epoch 20 - iter 343/496 - loss 0.86531508 - samples/sec: 13.52\n",
            "2020-04-17 17:53:38,864 epoch 20 - iter 392/496 - loss 0.86910323 - samples/sec: 13.33\n",
            "2020-04-17 17:55:39,336 epoch 20 - iter 441/496 - loss 0.87647221 - samples/sec: 13.02\n",
            "2020-04-17 17:57:33,068 epoch 20 - iter 490/496 - loss 0.87478208 - samples/sec: 13.80\n",
            "2020-04-17 17:57:47,102 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 17:57:47,103 EPOCH 20 done: loss 0.8726 - lr 0.1000\n",
            "2020-04-17 17:57:47,107 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 17:57:47,109 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 17:59:42,558 epoch 21 - iter 49/496 - loss 0.91007337 - samples/sec: 13.58\n",
            "2020-04-17 18:01:34,802 epoch 21 - iter 98/496 - loss 0.86923658 - samples/sec: 13.98\n",
            "2020-04-17 18:03:28,196 epoch 21 - iter 147/496 - loss 0.87137522 - samples/sec: 13.84\n",
            "2020-04-17 18:05:24,205 epoch 21 - iter 196/496 - loss 0.87110100 - samples/sec: 13.53\n",
            "2020-04-17 18:07:18,291 epoch 21 - iter 245/496 - loss 0.85668599 - samples/sec: 13.75\n",
            "2020-04-17 18:09:14,884 epoch 21 - iter 294/496 - loss 0.85243094 - samples/sec: 13.46\n",
            "2020-04-17 18:11:09,332 epoch 21 - iter 343/496 - loss 0.84825230 - samples/sec: 13.71\n",
            "2020-04-17 18:13:03,642 epoch 21 - iter 392/496 - loss 0.86642745 - samples/sec: 13.73\n",
            "2020-04-17 18:14:54,027 epoch 21 - iter 441/496 - loss 0.86271552 - samples/sec: 14.21\n",
            "2020-04-17 18:16:46,087 epoch 21 - iter 490/496 - loss 0.86438097 - samples/sec: 14.00\n",
            "2020-04-17 18:16:59,867 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 18:16:59,868 EPOCH 21 done: loss 0.8626 - lr 0.1000\n",
            "2020-04-17 18:16:59,870 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 18:16:59,872 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 18:18:55,609 epoch 22 - iter 49/496 - loss 0.76795835 - samples/sec: 13.55\n",
            "2020-04-17 18:20:53,717 epoch 22 - iter 98/496 - loss 0.84593349 - samples/sec: 13.29\n",
            "2020-04-17 18:22:45,705 epoch 22 - iter 147/496 - loss 0.85311753 - samples/sec: 14.01\n",
            "2020-04-17 18:24:39,452 epoch 22 - iter 196/496 - loss 0.84642587 - samples/sec: 13.79\n",
            "2020-04-17 18:26:31,163 epoch 22 - iter 245/496 - loss 0.85682819 - samples/sec: 14.05\n",
            "2020-04-17 18:28:27,647 epoch 22 - iter 294/496 - loss 0.85734236 - samples/sec: 13.47\n",
            "2020-04-17 18:30:24,860 epoch 22 - iter 343/496 - loss 0.84479720 - samples/sec: 13.39\n",
            "2020-04-17 18:32:21,939 epoch 22 - iter 392/496 - loss 0.85203204 - samples/sec: 13.40\n",
            "2020-04-17 18:34:13,277 epoch 22 - iter 441/496 - loss 0.84577118 - samples/sec: 14.09\n",
            "2020-04-17 18:36:08,906 epoch 22 - iter 490/496 - loss 0.84939900 - samples/sec: 13.57\n",
            "2020-04-17 18:36:24,749 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 18:36:24,750 EPOCH 22 done: loss 0.8496 - lr 0.1000\n",
            "2020-04-17 18:36:24,755 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 18:36:24,757 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 18:38:19,729 epoch 23 - iter 49/496 - loss 0.81550968 - samples/sec: 13.64\n",
            "2020-04-17 18:40:10,620 epoch 23 - iter 98/496 - loss 0.78595704 - samples/sec: 14.15\n",
            "2020-04-17 18:42:01,372 epoch 23 - iter 147/496 - loss 0.82169978 - samples/sec: 14.17\n",
            "2020-04-17 18:43:50,418 epoch 23 - iter 196/496 - loss 0.83035486 - samples/sec: 14.39\n",
            "2020-04-17 18:45:38,276 epoch 23 - iter 245/496 - loss 0.82086823 - samples/sec: 14.55\n",
            "2020-04-17 18:47:39,523 epoch 23 - iter 294/496 - loss 0.81913359 - samples/sec: 12.94\n",
            "2020-04-17 18:49:41,448 epoch 23 - iter 343/496 - loss 0.82588381 - samples/sec: 12.87\n",
            "2020-04-17 18:51:40,832 epoch 23 - iter 392/496 - loss 0.84073403 - samples/sec: 13.14\n",
            "2020-04-17 18:53:38,508 epoch 23 - iter 441/496 - loss 0.83904596 - samples/sec: 13.33\n",
            "2020-04-17 18:55:35,398 epoch 23 - iter 490/496 - loss 0.84116902 - samples/sec: 13.42\n",
            "2020-04-17 18:55:49,910 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 18:55:49,911 EPOCH 23 done: loss 0.8413 - lr 0.1000\n",
            "2020-04-17 18:55:49,915 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 18:55:49,917 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 18:57:48,618 epoch 24 - iter 49/496 - loss 0.85037839 - samples/sec: 13.21\n",
            "2020-04-17 18:59:44,809 epoch 24 - iter 98/496 - loss 0.85044111 - samples/sec: 13.50\n",
            "2020-04-17 19:01:40,740 epoch 24 - iter 147/496 - loss 0.84886421 - samples/sec: 13.54\n",
            "2020-04-17 19:03:39,540 epoch 24 - iter 196/496 - loss 0.81826899 - samples/sec: 13.21\n",
            "2020-04-17 19:05:38,372 epoch 24 - iter 245/496 - loss 0.82057394 - samples/sec: 13.20\n",
            "2020-04-17 19:07:35,903 epoch 24 - iter 294/496 - loss 0.83119003 - samples/sec: 13.35\n",
            "2020-04-17 19:09:35,683 epoch 24 - iter 343/496 - loss 0.82507521 - samples/sec: 13.10\n",
            "2020-04-17 19:11:33,423 epoch 24 - iter 392/496 - loss 0.82189387 - samples/sec: 13.33\n",
            "2020-04-17 19:13:30,270 epoch 24 - iter 441/496 - loss 0.82412983 - samples/sec: 13.43\n",
            "2020-04-17 19:15:28,417 epoch 24 - iter 490/496 - loss 0.81996947 - samples/sec: 13.28\n",
            "2020-04-17 19:15:40,919 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 19:15:40,920 EPOCH 24 done: loss 0.8219 - lr 0.1000\n",
            "2020-04-17 19:15:40,927 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 19:15:40,928 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 19:17:37,678 epoch 25 - iter 49/496 - loss 0.81641938 - samples/sec: 13.43\n",
            "2020-04-17 19:19:31,231 epoch 25 - iter 98/496 - loss 0.82950645 - samples/sec: 13.83\n",
            "2020-04-17 19:21:32,213 epoch 25 - iter 147/496 - loss 0.80347811 - samples/sec: 12.97\n",
            "2020-04-17 19:23:34,097 epoch 25 - iter 196/496 - loss 0.80212668 - samples/sec: 12.87\n",
            "2020-04-17 19:25:29,743 epoch 25 - iter 245/496 - loss 0.79106452 - samples/sec: 13.57\n",
            "2020-04-17 19:27:25,578 epoch 25 - iter 294/496 - loss 0.79882331 - samples/sec: 13.55\n",
            "2020-04-17 19:29:30,171 epoch 25 - iter 343/496 - loss 0.79770128 - samples/sec: 12.59\n",
            "2020-04-17 19:31:28,561 epoch 25 - iter 392/496 - loss 0.79124553 - samples/sec: 13.25\n",
            "2020-04-17 19:33:34,874 epoch 25 - iter 441/496 - loss 0.79206574 - samples/sec: 12.42\n",
            "2020-04-17 19:35:31,937 epoch 25 - iter 490/496 - loss 0.79425693 - samples/sec: 13.41\n",
            "2020-04-17 19:35:45,772 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 19:35:45,774 EPOCH 25 done: loss 0.7931 - lr 0.1000\n",
            "2020-04-17 19:35:45,777 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 19:35:45,779 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 19:37:41,008 epoch 26 - iter 49/496 - loss 0.73965646 - samples/sec: 13.61\n",
            "2020-04-17 19:39:34,530 epoch 26 - iter 98/496 - loss 0.76531387 - samples/sec: 13.82\n",
            "2020-04-17 19:41:36,846 epoch 26 - iter 147/496 - loss 0.79060066 - samples/sec: 12.83\n",
            "2020-04-17 19:43:40,882 epoch 26 - iter 196/496 - loss 0.80893372 - samples/sec: 12.65\n",
            "2020-04-17 19:45:41,692 epoch 26 - iter 245/496 - loss 0.81751223 - samples/sec: 12.99\n",
            "2020-04-17 19:47:41,636 epoch 26 - iter 294/496 - loss 0.81512955 - samples/sec: 13.08\n",
            "2020-04-17 19:49:41,844 epoch 26 - iter 343/496 - loss 0.80405953 - samples/sec: 13.05\n",
            "2020-04-17 19:51:39,648 epoch 26 - iter 392/496 - loss 0.80438659 - samples/sec: 13.32\n",
            "2020-04-17 19:53:42,290 epoch 26 - iter 441/496 - loss 0.81536629 - samples/sec: 12.79\n",
            "2020-04-17 19:55:44,706 epoch 26 - iter 490/496 - loss 0.82181600 - samples/sec: 12.82\n",
            "2020-04-17 19:55:59,911 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 19:55:59,913 EPOCH 26 done: loss 0.8221 - lr 0.1000\n",
            "2020-04-17 19:55:59,915 BAD EPOCHS (no improvement): 1\n",
            "2020-04-17 19:55:59,917 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 19:58:00,426 epoch 27 - iter 49/496 - loss 0.77025817 - samples/sec: 13.01\n",
            "2020-04-17 19:59:59,537 epoch 27 - iter 98/496 - loss 0.80366753 - samples/sec: 13.17\n",
            "2020-04-17 20:02:02,731 epoch 27 - iter 147/496 - loss 0.80058728 - samples/sec: 12.74\n",
            "2020-04-17 20:04:05,821 epoch 27 - iter 196/496 - loss 0.81519227 - samples/sec: 12.75\n",
            "2020-04-17 20:06:01,952 epoch 27 - iter 245/496 - loss 0.80468224 - samples/sec: 13.51\n",
            "2020-04-17 20:08:07,531 epoch 27 - iter 294/496 - loss 0.81928808 - samples/sec: 12.49\n",
            "2020-04-17 20:10:07,628 epoch 27 - iter 343/496 - loss 0.81076182 - samples/sec: 13.06\n",
            "2020-04-17 20:12:10,647 epoch 27 - iter 392/496 - loss 0.81127762 - samples/sec: 12.75\n",
            "2020-04-17 20:14:16,656 epoch 27 - iter 441/496 - loss 0.80645833 - samples/sec: 12.45\n",
            "2020-04-17 20:16:20,906 epoch 27 - iter 490/496 - loss 0.80503565 - samples/sec: 12.63\n",
            "2020-04-17 20:16:36,249 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 20:16:36,250 EPOCH 27 done: loss 0.8050 - lr 0.1000\n",
            "2020-04-17 20:16:36,256 BAD EPOCHS (no improvement): 2\n",
            "2020-04-17 20:16:36,259 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 20:18:42,307 epoch 28 - iter 49/496 - loss 0.80133136 - samples/sec: 12.44\n",
            "2020-04-17 20:20:47,833 epoch 28 - iter 98/496 - loss 0.79484512 - samples/sec: 12.50\n",
            "2020-04-17 20:22:48,387 epoch 28 - iter 147/496 - loss 0.78591053 - samples/sec: 13.02\n",
            "2020-04-17 20:24:48,525 epoch 28 - iter 196/496 - loss 0.77903142 - samples/sec: 13.06\n",
            "2020-04-17 20:26:58,909 epoch 28 - iter 245/496 - loss 0.78340390 - samples/sec: 12.03\n",
            "2020-04-17 20:29:05,620 epoch 28 - iter 294/496 - loss 0.78341244 - samples/sec: 12.38\n",
            "2020-04-17 20:31:06,632 epoch 28 - iter 343/496 - loss 0.79354345 - samples/sec: 12.97\n",
            "2020-04-17 20:33:07,771 epoch 28 - iter 392/496 - loss 0.79175608 - samples/sec: 12.95\n",
            "2020-04-17 20:35:06,083 epoch 28 - iter 441/496 - loss 0.78728814 - samples/sec: 13.26\n",
            "2020-04-17 20:37:11,603 epoch 28 - iter 490/496 - loss 0.78178761 - samples/sec: 12.50\n",
            "2020-04-17 20:37:25,998 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 20:37:25,999 EPOCH 28 done: loss 0.7793 - lr 0.1000\n",
            "2020-04-17 20:37:26,006 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 20:37:26,007 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 20:39:24,702 epoch 29 - iter 49/496 - loss 0.77519255 - samples/sec: 13.21\n",
            "2020-04-17 20:41:29,247 epoch 29 - iter 98/496 - loss 0.77551310 - samples/sec: 12.60\n",
            "2020-04-17 20:43:35,639 epoch 29 - iter 147/496 - loss 0.77047526 - samples/sec: 12.41\n",
            "2020-04-17 20:45:42,142 epoch 29 - iter 196/496 - loss 0.77341579 - samples/sec: 12.40\n",
            "2020-04-17 20:47:42,916 epoch 29 - iter 245/496 - loss 0.76854333 - samples/sec: 12.99\n",
            "2020-04-17 20:49:38,615 epoch 29 - iter 294/496 - loss 0.77041625 - samples/sec: 13.56\n",
            "2020-04-17 20:51:51,301 epoch 29 - iter 343/496 - loss 0.76118431 - samples/sec: 11.82\n",
            "2020-04-17 20:54:07,417 epoch 29 - iter 392/496 - loss 0.76179196 - samples/sec: 11.53\n",
            "2020-04-17 20:56:35,667 epoch 29 - iter 441/496 - loss 0.77826554 - samples/sec: 10.58\n",
            "2020-04-17 20:58:42,441 epoch 29 - iter 490/496 - loss 0.76657258 - samples/sec: 12.38\n",
            "2020-04-17 20:58:58,851 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 20:58:58,853 EPOCH 29 done: loss 0.7704 - lr 0.1000\n",
            "2020-04-17 20:58:58,857 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 20:58:58,859 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 21:01:03,317 epoch 30 - iter 49/496 - loss 0.80212958 - samples/sec: 12.60\n",
            "2020-04-17 21:03:14,393 epoch 30 - iter 98/496 - loss 0.78125831 - samples/sec: 11.97\n",
            "2020-04-17 21:05:27,699 epoch 30 - iter 147/496 - loss 0.76257893 - samples/sec: 11.77\n",
            "2020-04-17 21:07:45,876 epoch 30 - iter 196/496 - loss 0.77373339 - samples/sec: 11.35\n",
            "2020-04-17 21:09:58,389 epoch 30 - iter 245/496 - loss 0.77018379 - samples/sec: 11.84\n",
            "2020-04-17 21:12:04,541 epoch 30 - iter 294/496 - loss 0.76795226 - samples/sec: 12.44\n",
            "2020-04-17 21:14:06,665 epoch 30 - iter 343/496 - loss 0.76889170 - samples/sec: 12.85\n",
            "2020-04-17 21:16:12,479 epoch 30 - iter 392/496 - loss 0.77363108 - samples/sec: 12.47\n",
            "2020-04-17 21:18:10,909 epoch 30 - iter 441/496 - loss 0.77732584 - samples/sec: 13.25\n",
            "2020-04-17 21:20:13,118 epoch 30 - iter 490/496 - loss 0.78162446 - samples/sec: 12.84\n",
            "2020-04-17 21:20:28,586 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 21:20:28,587 EPOCH 30 done: loss 0.7800 - lr 0.1000\n",
            "2020-04-17 21:20:28,592 BAD EPOCHS (no improvement): 1\n",
            "2020-04-17 21:20:28,593 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 21:22:38,456 epoch 31 - iter 49/496 - loss 0.74834349 - samples/sec: 12.08\n",
            "2020-04-17 21:24:44,926 epoch 31 - iter 98/496 - loss 0.76708876 - samples/sec: 12.41\n",
            "2020-04-17 21:26:53,399 epoch 31 - iter 147/496 - loss 0.77579212 - samples/sec: 12.21\n",
            "2020-04-17 21:28:57,673 epoch 31 - iter 196/496 - loss 0.76390709 - samples/sec: 12.63\n",
            "2020-04-17 21:31:08,474 epoch 31 - iter 245/496 - loss 0.76019760 - samples/sec: 12.00\n",
            "2020-04-17 21:33:23,977 epoch 31 - iter 294/496 - loss 0.76523606 - samples/sec: 11.58\n",
            "2020-04-17 21:35:33,159 epoch 31 - iter 343/496 - loss 0.77617737 - samples/sec: 12.15\n",
            "2020-04-17 21:37:48,754 epoch 31 - iter 392/496 - loss 0.77743132 - samples/sec: 11.57\n",
            "2020-04-17 21:40:02,582 epoch 31 - iter 441/496 - loss 0.77468496 - samples/sec: 11.72\n",
            "2020-04-17 21:42:11,030 epoch 31 - iter 490/496 - loss 0.77259170 - samples/sec: 12.22\n",
            "2020-04-17 21:42:24,092 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 21:42:24,093 EPOCH 31 done: loss 0.7691 - lr 0.1000\n",
            "2020-04-17 21:42:24,097 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 21:42:24,098 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 21:44:28,676 epoch 32 - iter 49/496 - loss 0.75724352 - samples/sec: 12.59\n",
            "2020-04-17 21:46:35,622 epoch 32 - iter 98/496 - loss 0.72933127 - samples/sec: 12.36\n",
            "2020-04-17 21:48:39,233 epoch 32 - iter 147/496 - loss 0.75595141 - samples/sec: 12.69\n",
            "2020-04-17 21:50:53,779 epoch 32 - iter 196/496 - loss 0.77065003 - samples/sec: 11.66\n",
            "2020-04-17 21:53:02,298 epoch 32 - iter 245/496 - loss 0.76444684 - samples/sec: 12.21\n",
            "2020-04-17 21:55:13,482 epoch 32 - iter 294/496 - loss 0.75823245 - samples/sec: 11.96\n",
            "2020-04-17 21:57:30,539 epoch 32 - iter 343/496 - loss 0.75857001 - samples/sec: 11.45\n",
            "2020-04-17 21:59:44,987 epoch 32 - iter 392/496 - loss 0.75956487 - samples/sec: 11.67\n",
            "2020-04-17 22:01:54,252 epoch 32 - iter 441/496 - loss 0.75900095 - samples/sec: 12.14\n",
            "2020-04-17 22:04:05,022 epoch 32 - iter 490/496 - loss 0.75957633 - samples/sec: 12.00\n",
            "2020-04-17 22:04:19,265 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 22:04:19,266 EPOCH 32 done: loss 0.7580 - lr 0.1000\n",
            "2020-04-17 22:04:19,270 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 22:04:19,271 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 22:06:23,238 epoch 33 - iter 49/496 - loss 0.77208867 - samples/sec: 12.65\n",
            "2020-04-17 22:08:36,378 epoch 33 - iter 98/496 - loss 0.75892055 - samples/sec: 11.78\n",
            "2020-04-17 22:10:43,906 epoch 33 - iter 147/496 - loss 0.78201100 - samples/sec: 12.30\n",
            "2020-04-17 22:12:59,278 epoch 33 - iter 196/496 - loss 0.78492131 - samples/sec: 11.59\n",
            "2020-04-17 22:15:07,520 epoch 33 - iter 245/496 - loss 0.78210257 - samples/sec: 12.24\n",
            "2020-04-17 22:17:18,183 epoch 33 - iter 294/496 - loss 0.78101545 - samples/sec: 12.01\n",
            "2020-04-17 22:19:22,667 epoch 33 - iter 343/496 - loss 0.77825041 - samples/sec: 12.61\n",
            "2020-04-17 22:21:36,918 epoch 33 - iter 392/496 - loss 0.76368750 - samples/sec: 11.69\n",
            "2020-04-17 22:23:52,704 epoch 33 - iter 441/496 - loss 0.75699988 - samples/sec: 11.55\n",
            "2020-04-17 22:26:14,810 epoch 33 - iter 490/496 - loss 0.75786134 - samples/sec: 11.04\n",
            "2020-04-17 22:26:30,919 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 22:26:30,921 EPOCH 33 done: loss 0.7566 - lr 0.1000\n",
            "2020-04-17 22:26:30,922 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 22:26:30,923 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 22:28:50,104 epoch 34 - iter 49/496 - loss 0.69140863 - samples/sec: 11.27\n",
            "2020-04-17 22:31:04,958 epoch 34 - iter 98/496 - loss 0.72062712 - samples/sec: 11.63\n",
            "2020-04-17 22:33:27,481 epoch 34 - iter 147/496 - loss 0.71994070 - samples/sec: 11.01\n",
            "2020-04-17 22:35:46,273 epoch 34 - iter 196/496 - loss 0.72454379 - samples/sec: 11.30\n",
            "2020-04-17 22:38:02,260 epoch 34 - iter 245/496 - loss 0.73630017 - samples/sec: 11.54\n",
            "2020-04-17 22:40:12,107 epoch 34 - iter 294/496 - loss 0.73320915 - samples/sec: 12.08\n",
            "2020-04-17 22:42:27,958 epoch 34 - iter 343/496 - loss 0.72605194 - samples/sec: 11.55\n",
            "2020-04-17 22:44:39,707 epoch 34 - iter 392/496 - loss 0.72666419 - samples/sec: 11.91\n",
            "2020-04-17 22:46:55,522 epoch 34 - iter 441/496 - loss 0.73147401 - samples/sec: 11.55\n",
            "2020-04-17 22:49:09,891 epoch 34 - iter 490/496 - loss 0.73044164 - samples/sec: 11.68\n",
            "2020-04-17 22:49:24,683 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 22:49:24,684 EPOCH 34 done: loss 0.7313 - lr 0.1000\n",
            "2020-04-17 22:49:24,688 BAD EPOCHS (no improvement): 0\n",
            "2020-04-17 22:49:24,689 ----------------------------------------------------------------------------------------------------\n",
            "2020-04-17 22:51:33,783 epoch 35 - iter 49/496 - loss 0.67625919 - samples/sec: 12.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXEphnwWhQD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot weight traces \n",
        "plotter = Plotter()\n",
        "plotter.plot_weights('resources/taggers/example-ner/weights.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}